## Evaluating LLM on OpinionQA-XL Dataset

This README guides you through the process of evaluating a Large Language Model (LLM) on the OpinionQA-XL dataset, focusing on the "Representativeness" and "Steerability" metrics.

### 1. Generate Responses

Start by generating responses using the provided model. Execute the following command:

```bash
python get_log_prob_hf_xl.py <MODEL_NAME_OR_PATH>
```

- `<MODEL_NAME_OR_PATH>`: Specify the name of the LLM or the path to the model weights.

### 2. Evaluate Representativeness

After generating the responses, evaluate the "Representativeness" metric by running the script with the file generated from the previous command:

```bash
python eval_oqaxl_representativeness.py <OUTPUT_FILE>
```

- `<OUTPUT_FILE>`: Path to the file generated by the previous script.

### 3. Compute Steerability

To compute the "Steerability" metric, first generate model outputs for all demographics in parallel:

```bash
bash generate_OQAXL_responses.sh <MODEL_NAME_OR_PATH>
```

- `<MODEL_NAME_OR_PATH>`: Specify the name of the LLM or the path to the model weights.

This command will output a folder containing multiple JSON files.

Next, evaluate the "Steerability" using the script:

```bash
python eval_oqaxl_steerability.py <GENERATED_FOLDER_PATH>
```

- `<GENERATED_FOLDER_PATH>`: Path to the folder containing the generated JSON files.

If you use this evaluation method or the task files, please cite the work as:

```bibtex
@online{bhattacharyya2024align,
  author = {Bhattacharyya, Aanisha and Agrawal, Susmit and Singla, Yaman K and SR, Nikitha and Menta, Tarun Ram and Krishnamurthy, Balaji},
  title = {Measuring and Improving Persuasive Abilities of Generative Models},
  year = {2024},
  url = {https://behavior-in-the-wild.github.io/align-via-actions}
}
```
